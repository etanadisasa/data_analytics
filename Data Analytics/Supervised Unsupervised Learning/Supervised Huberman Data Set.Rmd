---
title: "Supervised Learning"
author: "Etana Disasa"
date: "12/13/2018"
output: html_document
---

## Loading and Exploring the Dataset
```{r fetchLibrary, include=FALSE}
library(class)
library(ggvis)
library(pander)
library(descr)
#library(gmodels)
library(caret)
library(lattice)
library(ggplot2)
library(data.table)
library(magrittr)

```
##### I was able to call libraries keep it hidden to avoid unnecessary details from displaying. 
### Loading Dataset
```{r dataFetching}
haberman <- read.csv("haberman.data.csv", header=FALSE)
setnames(haberman, old=c("V1","V2","V3","V4"), new=c("Age","Year","PositiveNodes","Status"))
haberman$Status[haberman$Status == 1] <- "Survived"
haberman$Status[haberman$Status == 2] <- "Deseased"
haberman$Status <- as.factor(haberman$Status)

summary(haberman)
```

### Exploring Data Set
```{r}
summary(haberman[c("Age","Year")])
summary(haberman[c("Age","PositiveNodes")])
summary(haberman[c("Year","PositiveNodes")])
```


## Scatter Plots of the Dataset
### Plot 1: Age and PositiveNodes

```{r dataPlot1}
haberman %>% ggvis(~Age,~Year,fill=~Status) %>% layer_points()
```

### Plot 2: Petal Width and Petal Length
```{r dataPlot2}
haberman %>% ggvis(~Age,~PositiveNodes, fill=~Status) %>% layer_points() 
```
### Plot 3: 
```{r dataPlot3}
haberman %>% ggvis(~Year,~PositiveNodes,fill=~Status) %>% layer_points()
```


##### Question: What does this show you? Is there positive or negative correlation between sepal length and width? The same way, in Plot 2, there positive or negative correlation between petal length and width?
##### Answer: These summaries do not give us conclusive results whether there is a positive or negative correlation among each groups. Therefore we cannot conclude anything definitely. 

## Training Dataset with 80/20%
### Preparing Data
```{r training1}
set.seed(3465)
ind <- sample(2,nrow(haberman), replace=TRUE, prob =c(0.8,0.2))
habermanTrain <- haberman[ind==1, 1:3]
habermanTest <- haberman[ind==2, 1:3]
habermanTrainLabels <- haberman[ind==1, 4]
habermanTestLabels <- haberman[ind==2, 4]
```
### Find the k-Nearest Neighbors of the Training Set
```{r}
haberman_pred <- knn(train = habermanTrain, test = habermanTest, cl = habermanTrainLabels, k=3)
haberman_pred
```
### Displaying Results
```{r dataPlot4}

display <-CrossTable(x = habermanTestLabels, y = haberman_pred, prop.chisq = FALSE)
pander(display, digits=1)
## Pander renders the CrossTable tables much better when RMarkdown file is knitting in html format. 
## Otherwise, it at times displays an error message (in pdf) or doesn't show it altogether. 
```
### Testing the Prediction
```{r testPrediction1}
# Put `irisTestLabels` in a data frame
habermanTestLabels <- data.frame(habermanTestLabels)
# Merge `iris_pred` and `iris.testLabels` 
merge <- data.frame(haberman_pred, habermanTestLabels)
# Specify column names for `merge`
names(merge) <- c("Predicted Status", "Observed/Recorded Status")
# Inspect `merge` 
merge
```
#### It appears that the prediction returned identical results with the species observed in all 20 predictions.
## When the Training Test ratio is 67/33%
### Training Data
```{r training2}
set.seed(3465) ##The exercise leaves setting the seed. Unless set here, the outcomes differ. 
ind <- sample(2, nrow(haberman), replace=TRUE, prob=c(0.67,0.33))
habermanTrain <- haberman[ind==1,1:3]
habermanTest <- haberman[ind==2,1:3]
habermanTrainLabels <- haberman[ind==1,4]
habermanTestLabels <- haberman[ind==2,4]
haberman_pred <- knn(train = habermanTrain, test=habermanTest, cl = habermanTrainLabels, k=3)
haberman_pred
```
### Plot Results
```{r dataplot5}
display <- CrossTable(x = habermanTestLabels, y = haberman_pred, prop.chisq = FALSE)
pander(display, digits=1)
```
### Testing the Prediction
```{r testPrediction2}
# Put `irisTestLabels` in a data frame
habermanTestLabels <- data.frame(habermanTestLabels)
# Merge `iris_pred` and `iris.testLabels` 
merge <- data.frame(haberman_pred, habermanTestLabels)
# Specify column names for `merge`
names(merge) <- c("Predicted Species", "Observed Species")
# Inspect `merge` 
merge
```
#### Here two wrong predictions were observed on rows 23rd and 25th by producing Virginica instead of Versicolor. 

### Normalize Dataset
#### The purpose of normilizing is to gize the KNN tool to better predict information. 
```{r normalize2}
normalize <- function(x) {
num <- x - min(x)
denom <- max(x) - min(x)
return (num/denom)
}
haberman_x <- as.data.frame(lapply(haberman[1:3], normalize))
summary(haberman)
summary(haberman_x)

## My attempt below is to see if I can get the normalized version of haberman from haberman_x by binding the fourth colument (Status)
haberman <- as.data.frame(cbind(haberman_x$Age,haberman_x$Year,haberman_x$PositiveNodes,haberman$Status))
setnames(haberman, old=c("V1","V2","V3","V4"), new=c("Age","Year","PositiveNodes","Status"))

haberman$Status[haberman$Status == 1] <- "Survived"
haberman$Status[haberman$Status == 2] <- "Deseased"
haberman$Status <- as.factor(haberman$Status)
summary(haberman) 
```

## Resources/References Utilized
https://www.datacamp.com/community/tutorials/machine-learning-in-r
https://stats.stackexchange.com/questions/86285/random-number-set-seedn-in-r
https://www.geeksforgeeks.org/regression-classification-supervised-machine-learning/
https://stackoverflow.com/questions/9251326/convert-data-frame-column-format-from-character-to-factor/9252447