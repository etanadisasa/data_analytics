---
title: "Supervised Learning"
author: "Etana Disasa"
date: "12/13/2018"
output: html_document
---
## Assignment Questions
#####Question 1. How do supervised learning algorithms solve regression and classification problems? (I am not wanting a description of the internal workings of the algorithms.)
#####Answer 1. Supervised learning algorithms solve regression problems by training from a dataset and execute numeric prediction. Classification problems are solved similarily but the prediction output is a categorical and descerete outcome.  
#####Question 2. What packages (in R, Python...) perform supervised learning?
#####Answer 2. R [mice, rpart, party, caret, randomForest], Python[Scikit, Tensorflow, Theano] 
#####Question 3. What measures of quality of the learning algorithm might you expect to see?
#####Answer 3. Primarily, a quality learning algorithm will pridict an output value that is closely similar or identical to the test dataset. 

## Loading and Exploring the Dataset
```{r fetchLibrary, include=FALSE}
library(class)
library(ggvis)
library(pander)
library(gmodels)
library(descr)
library(caret)
library(lattice)
library(ggplot2)
```
##### Libraries called and keep hidden to avoid unnecessary details from displaying. 
### Loading Dataset
```{r dataFetching}
data("iris")
summary(iris)
```
### Exploring Data Set
```{r}
summary(iris[c("Petal.Width","Sepal.Width")])
summary(iris[c("Petal.Length","Sepal.Length")])
```
## Scatter Plots of the Dataset
### Plot 1: Sepal Width and Sepal Length

```{r dataPlot1}
iris %>% ggvis(~Sepal.Length,~Sepal.Width,fill=~Species) %>% layer_points()
```

### Plot 2: Petal Width and Petal Length
```{r dataPlot2}
iris %>% ggvis(~Petal.Length,~Petal.Width, fill=~Species) %>% layer_points() 
```

##### Question: What does this show you? Is there positive or negative correlation between sepal length and width? The same way, in Plot 2, there positive or negative correlation between petal length and width?
##### Answer: In Plot 1, the is a clearer positive correlation between Sepal Width and Sepal Length in the Sesota species. Even though, the correlation is not very outstanding, the same positive correlation may be seen in the Versicolor and Virginica spcies. In Plot 2, as the size of a Petal Lengh increases, there is an increase of Petal Width which is positive correlation. Thus, both plots display positive correlation. 

## Training Dataset with 80/20%
### Preparing Data
```{r training1}
set.seed(3465)
ind <- sample(2,nrow(iris), replace=TRUE, prob =c(80,20))
irisTrain <- iris[ind==1, 1:4]
irisTest <- iris[ind==2, 1:4]
irisTrainLabels <- iris[ind==1, 5]
irisTestLabels <- iris[ind==2, 5]
```
### Find the k-Nearest Neighbors of the Training Set
```{r}
iris_pred <- knn(train = irisTrain, test = irisTest, cl = irisTrainLabels, k=3)
iris_pred
```
### Displaying Results
```{r dataPlot3}
CrossTable(x = irisTestLabels, y = iris_pred, prop.chisq = FALSE)
```
### Testing the Prediction
```{r testPrediction1}
# Put `irisTestLabels` in a data frame
irisTestLabels <- data.frame(irisTestLabels)
# Merge `iris_pred` and `iris.testLabels` 
merge <- data.frame(iris_pred, irisTestLabels)
# Specify column names for `merge`
names(merge) <- c("Predicted Species", "Observed Species")
# Inspect `merge` 
merge
```
#### It appears that the prediction returned identical results with the species observed in all 20 predictions.
## When the Training Test ratio is 67/33%
### Training Data
```{r training2}
set.seed(3465) ##The exercise leaves setting the seed. Unless set here, the outcomes differ. 
ind <- sample(2, nrow(iris), replace=TRUE, prob=c(0.67,0.33))
irisTrain <- iris[ind==1,1:4]
irisTest <- iris[ind==2,1:4]
irisTrainLabels <- iris[ind==1,5]
irisTestLabels <- iris[ind==2,5]
iris_pred <- knn(train = irisTrain, test=irisTest, cl = irisTrainLabels, k=3)
iris_pred
```
### Plot Results
```{r dataplot4}
display <- CrossTable(x = irisTestLabels, y = iris_pred, prop.chisq = FALSE)
pander(display, digits=1)
```
### Testing the Prediction
```{r testPrediction2}
# Put `irisTestLabels` in a data frame
irisTestLabels <- data.frame(irisTestLabels)
# Merge `iris_pred` and `iris.testLabels` 
merge <- data.frame(iris_pred, irisTestLabels)
# Specify column names for `merge`
names(merge) <- c("Predicted Species", "Observed Species")
# Inspect `merge` 
merge
```
#### Here two wrong predictions were observed on rows 23rd and 25th by producing Virginica instead of Versicolor. 
## Other Important Techniques I Exercised
### Creating Test and Training Datasets using 'createDataPartition' from 'caret' package
```{r training3}
set.seed(3465)
trainIndex <- createDataPartition(iris$Species, p=.8, list = FALSE, times = 1)
head(trainIndex)
irisTrain <-iris[trainIndex]
head(irisTrain)
irisTest <- iris[-trainIndex]
head(irisTest)
```

### Normalize Dataset
#### The purpose of normilizing is to gize the KNN tool to better predict information. 
```{r normalize}
normalize <- function(x) {
num <- x - min(x)
denom <- max(x) - min(x)
return (num/denom)
}
iris_x <- as.data.frame(lapply(iris[1:4], normalize))
summary(iris)
summary(iris_x)
```


## Resources/References Utilized
#####<https://www.datacamp.com/community/tutorials/machine-learning-in-r>
#####<https://stats.stackexchange.com/questions/86285/random-number-set-seedn-in-r>
#####<https://www.geeksforgeeks.org/regression-classification-supervised-machine-learning/>
#####<https://medium.freecodecamp.org/essential-libraries-for-machine-learning-in-python-82a9ada57aeb>