<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd"><html><head></head><body>











































<div class="container-fluid main-container">











<div class="fluid-row">



<h1 class="title toc-ignore">UnSupervised Learning</h1>
<h4 class="author"><em>Etana Disasa</em></h4>
<h4 class="date"><em>12/15/2018</em></h4>

</div>


<div class="section level5">
<h5>Question 1. How is unsupervised learning related to the statistical clustering problem?</h5>
</div>
<div class="section level5">
<h5>Answer 1. Unsupervised learning method uses cluter analysis method to find hidden patterns in a given data set, and group them into cluster. These grouping models use exploratory data analysis. Unsupervised learning, unlike Supervised learning, does not take input data to learn patterns.</h5>
</div>
<div class="section level5">
<h5>Question 2. What packages (in R, Python…) perform unsupervised learning?</h5>
</div>
<div class="section level5">
<h5>Answer 2: R [kmeans, NBClust, clusters], Pythong [kmeans, MeanShift, SpectraClustering, AgglomerativeClustering]</h5>
</div>
<div class="section level5">
<h5>Question 3. What measures of quality for the learning algorithm might you expect to see?</h5>
</div>
<div class="section level5">
<h5>Answer 3: It is my understanding that when we have either positive or negative correlations between data variables, the prediction can be more accurate.</h5>
</div>
<div class="section level2">
<h2>Exercise</h2>
<div class="section level4">
<h4>Loading the Iris Dataset into R and Verify Data</h4>
<pre class="r"><code>data(iris)
head(iris)</code></pre>
<pre><code>##   Sepal.Length Sepal.Width Petal.Length Petal.Width Species
## 1          5.1         3.5          1.4         0.2  setosa
## 2          4.9         3.0          1.4         0.2  setosa
## 3          4.7         3.2          1.3         0.2  setosa
## 4          4.6         3.1          1.5         0.2  setosa
## 5          5.0         3.6          1.4         0.2  setosa
## 6          5.4         3.9          1.7         0.4  setosa</code></pre>
</div>
<div class="section level3">
<h3>Set Seed, Apply Kmeans Function</h3>
<pre class="r"><code>set.seed(25)
summary(iris)</code></pre>
<pre><code>##   Sepal.Length    Sepal.Width     Petal.Length    Petal.Width   
##  Min.   :4.300   Min.   :2.000   Min.   :1.000   Min.   :0.100  
##  1st Qu.:5.100   1st Qu.:2.800   1st Qu.:1.600   1st Qu.:0.300  
##  Median :5.800   Median :3.000   Median :4.350   Median :1.300  
##  Mean   :5.843   Mean   :3.057   Mean   :3.758   Mean   :1.199  
##  3rd Qu.:6.400   3rd Qu.:3.300   3rd Qu.:5.100   3rd Qu.:1.800  
##  Max.   :7.900   Max.   :4.400   Max.   :6.900   Max.   :2.500  
##        Species  
##  setosa    :50  
##  versicolor:50  
##  virginica :50  
##                 
##                 
## </code></pre>
<pre class="r"><code>km &lt;- kmeans(iris[,1:4], 3,nstart=25)
km</code></pre>
<pre><code>## K-means clustering with 3 clusters of sizes 62, 38, 50
## 
## Cluster means:
##   Sepal.Length Sepal.Width Petal.Length Petal.Width
## 1     5.901613    2.748387     4.393548    1.433871
## 2     6.850000    3.073684     5.742105    2.071053
## 3     5.006000    3.428000     1.462000    0.246000
## 
## Clustering vector:
##   [1] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3
##  [36] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
##  [71] 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 2 2 2
## [106] 2 1 2 2 2 2 2 2 1 1 2 2 2 2 1 2 1 2 1 2 2 1 1 2 2 2 2 2 1 2 2 2 2 1 2
## [141] 2 2 1 2 2 2 1 2 2 1
## 
## Within cluster sum of squares by cluster:
## [1] 39.82097 23.87947 15.15100
##  (between_SS / total_SS =  88.4 %)
## 
## Available components:
## 
## [1] &quot;cluster&quot;      &quot;centers&quot;      &quot;totss&quot;        &quot;withinss&quot;    
## [5] &quot;tot.withinss&quot; &quot;betweenss&quot;    &quot;size&quot;         &quot;iter&quot;        
## [9] &quot;ifault&quot;</code></pre>
<pre class="r"><code>km$size </code></pre>
<pre><code>## [1] 62 38 50</code></pre>
<pre class="r"><code>## Added this line to see the sizes of each cluster. The sizes are 62/38/50. In a perfect prediction, these sizes would have been 50/50/50
table(km$cluster,iris$Species)</code></pre>
<pre><code>##    
##     setosa versicolor virginica
##   1      0         48        14
##   2      0          2        36
##   3     50          0         0</code></pre>
</div>
<div class="section level3">
<h3>Compare the Clusters with the Species and Plot Results</h3>
<pre class="r"><code>table(km$cluster,iris$Species)</code></pre>
<pre><code>##    
##     setosa versicolor virginica
##   1      0         48        14
##   2      0          2        36
##   3     50          0         0</code></pre>
<pre class="r"><code>plot(iris[,1], iris[,2], col=km$cluster)
points(km$centers[,c(1,2)], col=1:3, pch=8, cex =2)</code></pre>
<p><img src="javascript://" width="672"/></p>
<pre class="r"><code>plot(iris[,3], iris[,4], col=km$cluster)
points(km$centers[,c(3,4)], col=1:3, pch=8, cex=2)</code></pre>
<p><img src="javascript://" width="672"/></p>
</div>
</div>
<div class="section level2">
<h2>Assignment</h2>
<div class="section level3">
<h3>Loading the Haberman Dataset into R and Verify Data</h3>
<div class="section level5">
<h5>I have factorized the Status column and changed the observations into “Survived” and “Deseased” values.</h5>
<pre class="r"><code>library(data.table)
knitr::opts_chunk$set(echo = TRUE)
haberman &lt;- read.csv(&quot;haberman.data.csv&quot;, header=FALSE)
setnames(haberman, old=c(&quot;V1&quot;,&quot;V2&quot;,&quot;V3&quot;,&quot;V4&quot;), new=c(&quot;Age&quot;,&quot;Year&quot;,&quot;PositiveNodes&quot;,&quot;Status&quot;))
haberman$Status[haberman$Status == 1] &lt;- &quot;Survived&quot;
haberman$Status[haberman$Status == 2] &lt;- &quot;Deseased&quot;
haberman$Status &lt;- as.factor(haberman$Status)</code></pre>
</div>
</div>
<div class="section level3">
<h3>Set Seed, Apply K-means Function</h3>
<pre class="r"><code>set.seed(25)
summary(haberman)</code></pre>
<pre><code>##       Age             Year       PositiveNodes         Status   
##  Min.   :30.00   Min.   :58.00   Min.   : 0.000   Deseased: 81  
##  1st Qu.:44.00   1st Qu.:60.00   1st Qu.: 0.000   Survived:225  
##  Median :52.00   Median :63.00   Median : 1.000                 
##  Mean   :52.46   Mean   :62.85   Mean   : 4.026                 
##  3rd Qu.:60.75   3rd Qu.:65.75   3rd Qu.: 4.000                 
##  Max.   :83.00   Max.   :69.00   Max.   :52.000</code></pre>
<pre class="r"><code>km &lt;- kmeans(haberman[,1:3], 2, nstart=25)
km</code></pre>
<pre><code>## K-means clustering with 2 clusters of sizes 165, 141
## 
## Cluster means:
##        Age     Year PositiveNodes
## 1 44.26667 62.54545      3.793939
## 2 62.04255 63.21277      4.297872
## 
## Clustering vector:
##   [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
##  [36] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
##  [71] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
## [106] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
## [141] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 1 1 1 1 1 2 2 2 2 2 2 2 2
## [176] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2
## [211] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2
## [246] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2
## [281] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2
## 
## Within cluster sum of squares by cluster:
## [1] 14890.17 15616.85
##  (between_SS / total_SS =  44.1 %)
## 
## Available components:
## 
## [1] &quot;cluster&quot;      &quot;centers&quot;      &quot;totss&quot;        &quot;withinss&quot;    
## [5] &quot;tot.withinss&quot; &quot;betweenss&quot;    &quot;size&quot;         &quot;iter&quot;        
## [9] &quot;ifault&quot;</code></pre>
</div>
<div class="section level3">
<h3>Compare the Clusters with the Species and Plot Results</h3>
<pre class="r"><code>table(km$cluster,haberman$Status)</code></pre>
<pre><code>##    
##     Deseased Survived
##   1       43      122
##   2       38      103</code></pre>
<pre class="r"><code>km$size ## Added the cluster sizes for comparison with the above table. </code></pre>
<pre><code>## [1] 165 141</code></pre>
<div class="section level5">
<h5>The first cluster contains 165 observations and predicted 43 “Deseased” and 122 “Survived”. It was supposed to contain 225 observations of all “Survived” Status. The 43 “Deseased” were predicted by error.</h5>
</div>
<div class="section level5">
<h5>The second cluster contains 141 obersations of which 38 are predicted as “Deseased” and 103 as “Survived”. Nevertheless, it was supposed be 81 predictions of all “Deseased” Status observations. Therefore, it had wrongfully predicted 103 observations as “Survived”</h5>
<pre class="r"><code>plot(haberman[,1], haberman[,2], col=km$cluster) #Plots by Age, Year
points(km$centers[,c(1,2)], col=1:2, pch=8, cex =3) </code></pre>
<p><img src="javascript://" width="672"/></p>
<pre class="r"><code>plot(haberman[,1], haberman[,3], col=km$cluster) #Plots by Age, PositiveNodes
points(km$centers[,c(1,3)], col=1:2, pch=8, cex=2)</code></pre>
<p><img src="javascript://" width="672"/></p>
<pre class="r"><code>plot(haberman[,2], haberman[,3], col=km$cluster) #Plots by Year, PositiveNodes
points(km$centers[,c(2,3)], col=1:2, pch=8, cex=2)</code></pre>
<p><img src="javascript://" width="672"/></p>
</div>
<div class="section level5">
<h5>To conclude here, I would say, because there is not any linear correlations (+ve, or -ve) between the Age, Year and PositiveNodes, the prediction was not very promising.</h5>
</div>
</div>
<div class="section level3">
<h3>Resources</h3>
<div class="section level5">
<h5><a rel="noopener" href="https://www.mathworks.com/discovery/unsupervised-learning.html" class="uri">https://www.mathworks.com/discovery/unsupervised-learning.html</a></h5>
</div>
<div class="section level5">
<h5><a rel="noopener" href="https://www.datanovia.com/en/blog/types-of-clustering-methods-overview-and-quick-start-r-code/" class="uri">https://www.datanovia.com/en/blog/types-of-clustering-methods-overview-and-quick-start-r-code/</a></h5>
</div>
<div class="section level5">
<h5><a rel="noopener" href="https://hdbscan.readthedocs.io/en/latest/comparing_clustering_algorithms.html" class="uri">https://hdbscan.readthedocs.io/en/latest/comparing_clustering_algorithms.html</a></h5>
</div>
<div class="section level5">
<h5><a rel="noopener" href="https://towardsdatascience.com/data-correlation-can-make-or-break-your-machine-learning-project-82ee11039cc9" class="uri">https://towardsdatascience.com/data-correlation-can-make-or-break-your-machine-learning-project-82ee11039cc9</a></h5>
</div>
</div>
</div>




</div>








<script type="text/javascript" src="/d2l/common/math/MathML.js?v=20.22.3.35896 "></script><script type="text/javascript">document.addEventListener('DOMContentLoaded', function() { D2LMathML.DesktopInit('https://s.brightspace.com/lib/mathjax/2.7.4/MathJax.js?config=MML_HTMLorMML','https://s.brightspace.com/lib/mathjax/2.7.4/MathJax.js?config=TeX-AMS-MML_HTMLorMML','130',false); });</script></body></html>